{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Student Feedback Data Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"analytics.jpg\" width=\"500\" height=\"100\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import xlrd\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nl\n",
    "nl.download('punkt',quiet=True)\n",
    "nl.download('stopwords',quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import collections\n",
    "from nltk.text import Text\n",
    "import re \n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from operator import itemgetter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from ipywidgets import Button, HBox, VBox, Layout, Box\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis.sklearn\n",
    "from pivottablejs import pivot_ui\n",
    "import os\n",
    "import sys \n",
    "import tkinter as tk\n",
    "from pandas.api.types import is_string_dtype\n",
    "import sys\n",
    "sys.tracebacklimit=0\n",
    "# Modify the path \n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import yellowbrick as yb\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Imports\n",
    "##########################################################################\n",
    "\n",
    "from yellowbrick.text.base import TextVisualizer\n",
    "\n",
    "##########################################################################\n",
    "# PosTagVisualizer\n",
    "##########################################################################\n",
    "\n",
    "class PosTagVisualizer(TextVisualizer):\n",
    "    \"\"\"\n",
    "    A part-of-speech tag visualizer colorizes text to enable\n",
    "    the user to visualize the proportions of nouns, verbs, etc.\n",
    "    and to use this information to make decisions about text\n",
    "    normalization (e.g. stemming vs lemmatization) and\n",
    "    vectorization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs : dict\n",
    "        Pass any additional keyword arguments to the super class.\n",
    "    cmap : dict\n",
    "        ANSII colormap\n",
    "\n",
    "    These parameters can be influenced later on in the visualization\n",
    "    process, but can and should be set as early as possible.\n",
    "    \"\"\"\n",
    "    def __init__(self, ax=None, **kwargs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        super(PosTagVisualizer, self).__init__(ax=ax, **kwargs)\n",
    "\n",
    "        # TODO: hard-coding in the ANSII colormap for now.\n",
    "        # Can we let the user reset the colors here?\n",
    "        self.COLORS = {\n",
    "            'white'      : \"\\033[0;37m{}\\033[0m\",\n",
    "            'yellow'     : \"\\033[0;33m{}\\033[0m\",\n",
    "            'green'      : \"\\033[0;32m{}\\033[0m\",\n",
    "            'blue'       : \"\\033[0;34m{}\\033[0m\",\n",
    "            'cyan'       : \"\\033[0;36m{}\\033[0m\",\n",
    "            'red'        : \"\\033[0;31m{}\\033[0m\",\n",
    "            'magenta'    : \"\\033[0;35m{}\\033[0m\",\n",
    "            'black'      : \"\\033[0;30m{}\\033[0m\",\n",
    "            'darkwhite'  : \"\\033[1;37m{}\\033[0m\",\n",
    "            'darkyellow' : \"\\033[1;33m{}\\033[0m\",\n",
    "            'darkgreen'  : \"\\033[1;32m{}\\033[0m\",\n",
    "            'darkblue'   : \"\\033[1;34m{}\\033[0m\",\n",
    "            'darkcyan'   : \"\\033[1;36m{}\\033[0m\",\n",
    "            'darkred'    : \"\\033[1;31m{}\\033[0m\",\n",
    "            'darkmagenta': \"\\033[1;35m{}\\033[0m\",\n",
    "            'darkblack'  : \"\\033[1;30m{}\\033[0m\",\n",
    "             None        : \"\\033[0;0m{}\\033[0m\"\n",
    "        }\n",
    "        \n",
    "        self.TAGS = {\n",
    "            'NN'   : 'green',\n",
    "            'NNS'  : 'green',\n",
    "            'NNP'  : 'green',\n",
    "            'NNPS' : 'green',\n",
    "            'VB'   : 'blue',\n",
    "            'VBD'  : 'blue',\n",
    "            'VBG'  : 'blue',\n",
    "            'VBN'  : 'blue',\n",
    "            'VBP'  : 'blue',\n",
    "            'VBZ'  : 'blue',\n",
    "            'JJ'   : 'red',\n",
    "            'JJR'  : 'red',\n",
    "            'JJS'  : 'red',\n",
    "            'RB'   : 'cyan',\n",
    "            'RBR'  : 'cyan',\n",
    "            'RBS'  : 'cyan',\n",
    "            'IN'   : 'darkwhite',\n",
    "            'POS'  : 'darkyellow',\n",
    "            'PRP$' : 'magenta',\n",
    "            'PRP$' : 'magenta',\n",
    "            'DT'   : 'black',\n",
    "            'CC'   : 'black',\n",
    "            'CD'   : 'black',\n",
    "            'WDT'  : 'black',\n",
    "            'WP'   : 'black',\n",
    "            'WP$'  : 'black',\n",
    "            'WRB'  : 'black',\n",
    "            'EX'   : 'yellow',\n",
    "            'FW'   : 'yellow',\n",
    "            'LS'   : 'yellow',\n",
    "            'MD'   : 'yellow',\n",
    "            'PDT'  : 'yellow',\n",
    "            'RP'   : 'yellow',\n",
    "            'SYM'  : 'yellow',\n",
    "            'TO'   : 'yellow',\n",
    "            'None' : 'off'\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    def colorize(self, token, color):\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        return self.COLORS[color].format(token)\n",
    "\n",
    "    def transform(self, tagged_tuples):\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.tagged = [\n",
    "            (self.TAGS.get(tag),tok) for tok, tag in tagged_tuples\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSP:\n",
    "    def __init__(self):\n",
    "        self.words = ['Data Settings','Explore','Visualize', 'WordClouds','Topic Modelling','Sentiment Analysis','Text Summarization']\n",
    "        self.items = [Button(description=w,button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            layout=Layout(width='175px')) for w in self.words]\n",
    "        display(HBox([item for item in self.items]))\n",
    "        np.warnings.filterwarnings('ignore')\n",
    "        self.import_data=widgets.Button(\n",
    "        description='Import New Data',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Click to import new data',\n",
    "        icon='check'\n",
    "        )\n",
    "        display(self.import_data)\n",
    "        self.file_path=\"\"\n",
    "        self.sheet=\"\"\n",
    "        self.att=\"\"\n",
    "        self.org_df=\"\"\n",
    "        self.import_data.on_click(self.new_data)\n",
    "        self.items[0].on_click(self.preview_data)\n",
    "        self.items[1].on_click(self.explore_data)\n",
    "        self.items[2].on_click(self.vis_data)\n",
    "        self.items[3].on_click(self.wc_data)\n",
    "        self.items[4].on_click(self.tm_data)\n",
    "        self.items[5].on_click(self.sa_data)\n",
    "        self.items[6].on_click(self.ts_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def new_data(self,b):\n",
    "        from tkinter import filedialog\n",
    "        \n",
    "\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "\n",
    "        self.file_path = filedialog.askopenfilename()\n",
    "        self.labelf=widgets.Label(value=\"Selected Data : \"+str(self.file_path),layout=Layout(width='50%'))\n",
    "        self.org_df=pd.read_excel(self.file_path)\n",
    "        clear_output()\n",
    "        display(HBox([item for item in self.items]))\n",
    "        display(self.import_data)\n",
    "        display(self.labelf)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def preview_data(self,b):\n",
    "        if len(self.file_path)==0:\n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            display(\"Please import new data\")\n",
    "        else:          \n",
    "            \n",
    "            \n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            self.wb = xlrd.open_workbook(self.file_path) \n",
    "            self.sheets=self.wb.sheet_names()\n",
    "            self.sheet_to_index={}\n",
    "            i=0\n",
    "            while i<len(self.sheets):\n",
    "                self.sheet_to_index[self.sheets[i]]=i\n",
    "                i=i+1\n",
    "            self.sheet_drop=widgets.Dropdown(options=self.sheets,value=self.sheets[0],layout=Layout(width='50%'),description='Sheet:',tooltip='Select a working sheet',disabled=False,)\n",
    "\n",
    "\n",
    "            self.sheet=self.wb.sheet_by_index(self.sheet_to_index[self.sheet_drop.value])\n",
    "            self.cols=[]\n",
    "            for i in range(self.sheet.ncols): \n",
    "                self.cols.append(self.sheet.cell_value(0, i)) \n",
    "            self.att_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature for data cleaning and analysis',disabled=False,)\n",
    "            self.att=self.att_drop.value\n",
    "\n",
    "\n",
    "            self.item_display=[]\n",
    "            self.item_display.append(self.labelf)\n",
    "            self.item_display.append(self.sheet_drop)\n",
    "            self.item_display.append(self.att_drop)\n",
    "            display(HBox([item for item in self.item_display]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            #print(\"hi\")\n",
    "            \n",
    "            \n",
    "            self.clean_choice=widgets.ToggleButtons(\n",
    "            options=['Original', 'Cleaned',],    \n",
    "            value='Cleaned',\n",
    "            description='Use Data:',\n",
    "            layout=Layout(width='50%'),\n",
    "            disabled=False,\n",
    "            button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Description',\n",
    "\n",
    "            )\n",
    "            display(self.clean_choice)\n",
    "            \n",
    "            self.df = pd.read_excel(self.file_path)\n",
    "            self.rem_value=widgets.Text(value='-No Answer-',placeholder='Type missing value indicator',layout=Layout(width='50%'),description='Missing value:',disabled=False)\n",
    "            if self.clean_choice.value=='Cleaned':\n",
    "\n",
    "                    self.df_cleaned=self.df.dropna()\n",
    "                    self.rem_value=widgets.Text(value='-No Answer-',placeholder='Type missing value indicator',layout=Layout(width='50%'),description='Missing value:',disabled=False)\n",
    "                    display(self.rem_value)\n",
    "                    self.df_cleaned = self.df_cleaned[self.df_cleaned[self.att_drop.value] != self.rem_value.value]\n",
    "                    self.df_cleaned = self.df_cleaned.reset_index(drop=True)\n",
    "                    self.org_df=self.df_cleaned\n",
    "                    print(\"\\nData cleaned by removing NaN values and rows with value = \"+ self.rem_value.value + \" in the column \" + self.att_drop.value) \n",
    "                    print(\"\\nA peek into cleaned data\\n\")\n",
    "                    print(\"\\nThere are {} observations and {} features in cleaned dataset \\n\".format(self.df_cleaned.shape[0],self.df_cleaned.shape[1]))\n",
    "                    display(self.df_cleaned.head())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            def choice_disp(change):\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(HBox([item for item in self.item_display]))\n",
    "                display(self.clean_choice)\n",
    "\n",
    "                if self.clean_choice.value=='Original':\n",
    "\n",
    "                    #print(\"\\nThere are {} observations and {} features in original dataset and {} observations and {} features in cleaned dataset. \\n\".format(df.shape[0],df.shape[1],df_cleaned.shape[0],df_cleaned.shape[1])) \n",
    "                    print(\"\\nA peek into originl data\\n\")\n",
    "                    print(\"\\nThere are {} observations and {} features in original dataset \\n\".format(self.df.shape[0],self.df.shape[1]))\n",
    "                    self.org_df=self.df\n",
    "                    display(self.df.head())\n",
    "\n",
    "                else:\n",
    "                    if self.clean_choice.value=='Cleaned':\n",
    "                        self.df_cleaned=self.df.dropna()\n",
    "\n",
    "                        display(self.rem_value)\n",
    "                        self.df_cleaned = self.df_cleaned[self.df_cleaned[self.att_drop.value] != self.rem_value.value]\n",
    "                        self.df_cleaned = self.df_cleaned.reset_index(drop=True)\n",
    "                        self.org_df=self.df_cleaned\n",
    "                        self.att=self.att_drop.value\n",
    "                        print(\"\\nData cleaned by removing NaN values and rows with value = \"+ self.rem_value.value + \" in the column \" + self.att_drop.value) \n",
    "                        print(\"\\nA peek into cleaned data\\n\")\n",
    "                        print(\"\\nThere are {} observations and {} features in cleaned dataset \\n\".format(self.df_cleaned.shape[0],self.df_cleaned.shape[1]))\n",
    "                        display(self.df_cleaned.head())\n",
    "\n",
    "\n",
    "\n",
    "            self.clean_choice.observe(choice_disp,'value')\n",
    "            self.rem_value.on_submit(choice_disp)\n",
    "            self.att_drop.observe(choice_disp, 'value')\n",
    "            \n",
    "            def sheet_disp(change):\n",
    "                \n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                self.sheet=self.wb.sheet_by_index(self.sheet_to_index[self.sheet_drop.value])\n",
    "                self.cols=[]\n",
    "                for i in range(self.sheet.ncols): \n",
    "                    self.cols.append(self.sheet.cell_value(0, i)) \n",
    "                self.att_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature for data cleaning and analysis',disabled=False,)\n",
    "                self.att=self.att_drop.value\n",
    "\n",
    "\n",
    "                self.item_display=[]\n",
    "                self.item_display.append(self.labelf)\n",
    "                self.item_display.append(self.sheet_drop)\n",
    "                self.item_display.append(self.att_drop)\n",
    "                display(HBox([item for item in self.item_display]))\n",
    "                self.clean_choice=widgets.ToggleButtons(\n",
    "                options=['Original', 'Cleaned',],    \n",
    "                value='Cleaned',\n",
    "                description='Use Data:',\n",
    "                layout=Layout(width='50%'),\n",
    "                disabled=False,\n",
    "                button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                tooltip='Description',\n",
    "\n",
    "                )\n",
    "                display(self.clean_choice)\n",
    "\n",
    "                self.df = pd.read_excel(self.file_path)\n",
    "                self.rem_value=widgets.Text(value='-No Answer-',placeholder='Type missing value indicator',layout=Layout(width='50%'),description='Missing value:',disabled=False)\n",
    "                if self.clean_choice.value=='Cleaned':\n",
    "\n",
    "                        self.df_cleaned=self.df.dropna()\n",
    "                        self.rem_value=widgets.Text(value='-No Answer-',placeholder='Type missing value indicator',layout=Layout(width='50%'),description='Missing value:',disabled=False)\n",
    "                        display(self.rem_value)\n",
    "                        self.df_cleaned = self.df_cleaned[self.df_cleaned[self.att_drop.value] != self.rem_value.value]\n",
    "                        self.df_cleaned = self.df_cleaned.reset_index(drop=True)\n",
    "                        self.org_df=self.df_cleaned\n",
    "                        self.att=self.att_drop.value\n",
    "                        print(\"\\nData cleaned by removing NaN values and rows with value = \"+ self.rem_value.value + \" in the column \" + self.att_drop.value) \n",
    "                        print(\"\\nA peek into cleaned data\\n\")\n",
    "                        print(\"\\nThere are {} observations and {} features in cleaned dataset \\n\".format(self.df_cleaned.shape[0],self.df_cleaned.shape[1]))\n",
    "                        display(self.df_cleaned.head())\n",
    "                        \n",
    "                self.clean_choice.observe(choice_disp,'value')\n",
    "                self.rem_value.on_submit(choice_disp)\n",
    "                self.att_drop.observe(choice_disp, 'value')\n",
    "\n",
    "            \n",
    "            \n",
    "            self.sheet_drop.observe(sheet_disp, 'value')\n",
    "        \n",
    "    def explore_data(self,b):\n",
    "                \n",
    "        if self.sheet==None or self.sheet==\"\":\n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            display(\"Please change data settings before use\")\n",
    "                \n",
    "        else: \n",
    "            \n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            self.action_list=['View Data','Sort Data','Group Data by Feature']\n",
    "            self.action_drop=widgets.Dropdown(options=self.action_list,value=self.action_list[0],layout=Layout(width='50%'),description='Action:',tooltip='Select an action to perform',disabled=False,)\n",
    "            display(self.action_drop)\n",
    "            display(self.org_df)\n",
    "            \n",
    "            def action(change):\n",
    "                flag=0\n",
    "                def change_order(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.action_drop)\n",
    "                    display(self.sort_order)\n",
    "                    if self.sort_order.value=='Ascending':\n",
    "                        flag=0\n",
    "                        self.sort_df = self.org_df.sort_values(by = self.att)\n",
    "                        display(self.sort_df)\n",
    "                    else: \n",
    "                        flag=1\n",
    "                        self.sort_df = self.org_df.sort_values(by = self.att,ascending=False)\n",
    "                        display(self.sort_df)\n",
    "                        \n",
    "                        \n",
    "                \n",
    "                        \n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(self.action_drop)\n",
    "                if self.action_drop.value=='View Data':\n",
    "                    display(self.org_df)\n",
    "                \n",
    "                if self.action_drop.value=='Sort Data':\n",
    "                    self.sort_order=widgets.RadioButtons(\n",
    "                        options=['Ascending', 'Descending'],\n",
    "                        value='Ascending',\n",
    "                        description='Sorting order:',\n",
    "                        disabled=False\n",
    "                    )\n",
    "                    display(self.sort_order)\n",
    "                    self.sort_df = self.org_df.sort_values(by = self.att)\n",
    "                    display(self.sort_df)\n",
    "                    self.sort_order.observe(change_order,'value')\n",
    "                    \n",
    "                    \n",
    "                                            \n",
    "                if self.action_drop.value=='Group Data by Feature':\n",
    "                    self.feat = self.org_df.groupby(self.att_drop.value)\n",
    "                    display(self.feat.describe())\n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            self.action_drop.observe(action, 'value')\n",
    "            #display(HBox([item for item in self.item_display]))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def vis_data(self, b):\n",
    "        if self.sheet==None or self.sheet==\"\":\n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            display(\"Please change data settings before use\")\n",
    "                \n",
    "        else: \n",
    "            \n",
    "            clear_output()\n",
    "            display(HBox([item for item in self.items]))\n",
    "            display(self.import_data)\n",
    "            display(widgets.Label(value=\"Visualise feature distribution through Bar and Pie Chart\",layout=Layout(width='50%')))\n",
    "            \n",
    "            \n",
    "            self.plotfeat_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to plot distribution',disabled=False,)\n",
    "            display(self.plotfeat_drop)\n",
    "            entire_feature = self.org_df.groupby(self.plotfeat_drop.value)\n",
    "            #plot graph of selected distribution\n",
    "            _labels =self.org_df[self.plotfeat_drop.value].unique()\n",
    "            plt.figure(figsize=(20,20))\n",
    "            ax = plt.subplot(221)\n",
    "            ax.set_aspect(1)\n",
    "\n",
    "            entire_feature.size().sort_values(ascending=False).plot.pie(labels = _labels, autopct='%1.1f%%',legend = True, fontsize=20)\n",
    "            plt.ylabel('')\n",
    "            plt.title(str(self.plotfeat_drop.value)+' Distribution')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            \n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.subplot(222)\n",
    "            entire_feature.size().sort_values(ascending=False).plot.bar(legend = True,fontsize=20)\n",
    "            plt.xticks(rotation=50)\n",
    "            plt.xticks(np.arange(2), _labels)\n",
    "            plt.ylabel('')\n",
    "            plt.xlabel('')\n",
    "            #might need to come up with a better graph title\n",
    "            plt.title(str(self.plotfeat_drop.value)+' Distribution')\n",
    "            plt.subplots_adjust(bottom=0.1, right=1.5, top=0.9)\n",
    "            plt.show()  \n",
    "            \n",
    "            def pie_plot(change):\n",
    "                \n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                \n",
    "                display(widgets.Label(value=\"Visualise feature distribution through Bar and Pie Chart\",layout=Layout(width='50%')))\n",
    "            \n",
    "                display(self.plotfeat_drop)\n",
    "                entire_feature = self.org_df.groupby(self.plotfeat_drop.value)\n",
    "                #plot graph of selected distribution\n",
    "                _labels =self.org_df[self.plotfeat_drop.value].unique()\n",
    "                \n",
    "                #print(_labels)\n",
    "                plt.figure(figsize=(20,20))\n",
    "                ax = plt.subplot(221)\n",
    "                ax.set_aspect(1)\n",
    "\n",
    "                entire_feature.size().sort_values(ascending=False).plot.pie(labels = _labels, autopct='%1.1f%%',legend = True, fontsize=20)\n",
    "                plt.ylabel('')\n",
    "                plt.title(str(self.plotfeat_drop.value)+' Distribution')\n",
    "                plt.grid(True)\n",
    "                \n",
    "                \n",
    "              \n",
    "                plt.figure(figsize=(20,20))\n",
    "                plt.subplot(222)\n",
    "                entire_feature.size().sort_values(ascending=False).plot.bar(legend = True,fontsize=20)\n",
    "                plt.xticks(rotation=50)\n",
    "                plt.xticks(np.arange(2), _labels)\n",
    "                plt.ylabel('')\n",
    "                plt.xlabel('')\n",
    "                #might need to come up with a better graph title\n",
    "                plt.title(str(self.plotfeat_drop.value)+' Distribution')\n",
    "                plt.subplots_adjust(bottom=0.1, right=1.5, top=0.9)\n",
    "                plt.show()               \n",
    "                \n",
    "                \n",
    "                \n",
    "            self.plotfeat_drop.observe(pie_plot,'value')\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    def wc_data(self,b):\n",
    "        try:\n",
    "            if self.sheet==None or self.sheet==\"\":\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(\"Please change data settings before use\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Visualise Text through Word Clouds. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                self.wc_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.wc_drop)\n",
    "                #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "\n",
    "                #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                    #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "                all_reviews=self.org_df[self.wc_drop.value]\n",
    "                #tokenization,removing stopwords, punctuation and stemming\n",
    "                all_ans=\"\"\n",
    "                for review in all_reviews:\n",
    "                    all_ans=all_ans+review+\"\\n\"\n",
    "                all_ans= all_ans.replace(\"'\", \"\")\n",
    "                tokens=word_tokenize(all_ans)\n",
    "                tokens=[w.lower() for w in tokens]\n",
    "                text = nl.Text(tokens)\n",
    "                token_words=[word for word in tokens if word.isalpha()]\n",
    "                stopword=stopwords.words('english')\n",
    "                stopword.append('dont')\n",
    "                stopword.append('didnt')\n",
    "                stopword.append('doesnt')\n",
    "                stopword.append('cant')\n",
    "                stopword.append('couldnt')\n",
    "                stopword.append('couldve')\n",
    "                stopword.append('im')\n",
    "                stopword.append('ive')\n",
    "                stopword.append('isnt')\n",
    "                stopword.append('theres')\n",
    "                stopword.append('wasnt')\n",
    "                stopword.append('wouldnt')\n",
    "                stopword.append('a')\n",
    "                stopword.append('also')\n",
    "                token_words=[w for w in token_words if not w in stopword]\n",
    "                porter =PorterStemmer()\n",
    "                token_stemmed=[porter.stem(w) for w in token_words]\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))\n",
    "                #creating worlcloud\n",
    "                #for w in token_words:\n",
    "                    #if type(w)!=\"<class 'str'>\":\n",
    "                        #print(type(w))\n",
    "                        #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "\n",
    "                        \n",
    "                cloudstring=(\" \").join(token_words)\n",
    "            \n",
    "                wordcloud = WordCloud(max_font_size=50,max_words=100, background_color=\"black\").generate(cloudstring)\n",
    "                plt.figure(figsize=(20,20))\n",
    "                ax = plt.subplot(221)\n",
    "                # plot wordcloud in matplotlib\n",
    "                plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(\"WordCloud\")\n",
    "                plt.grid(True)\n",
    "                #plotting bi-gram cloud\n",
    "                # setup and score the bigrams using the raw frequency.\n",
    "                finder = BigramCollocationFinder.from_words(token_words)\n",
    "                bigram_measures = BigramAssocMeasures()\n",
    "                scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "                scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    "                word_dict = {} \n",
    "                listLen = len(scoredList)\n",
    "                for i in range(listLen):\n",
    "                    word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "                wordCloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\")\n",
    "                plt.subplot(222) \n",
    "                wordCloud.generate_from_frequencies(word_dict)\n",
    "\n",
    "                plt.title('Most frequently occurring bigrams connected with an underscore_')\n",
    "                plt.imshow(wordCloud, interpolation='bilinear')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "                #plotting frequency distribution\n",
    "                plt.figure(figsize=(25,5))\n",
    "                ax = plt.subplot(121)\n",
    "                freqdist = nl.FreqDist(token_words)\n",
    "                plt.subplot(121) \n",
    "                plt.title(\"Frequency Distribution of top 50 token words\")\n",
    "                freqdist.plot(50)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                def wc_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Visualise Text through Word Clouds. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.wc_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.wc_drop)\n",
    "                    #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "                    #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                        #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    " \n",
    "\n",
    "                    all_reviews=self.org_df[self.wc_drop.value]\n",
    "                    #tokenization,removing stopwords, punctuation and stemming\n",
    "                    all_ans=\"\"\n",
    "                    for review in all_reviews:\n",
    "                        all_ans=all_ans+review+\"\\n\"\n",
    "                    all_ans= all_ans.replace(\"'\", \"\")\n",
    "                    tokens=word_tokenize(all_ans)\n",
    "                    tokens=[w.lower() for w in tokens]\n",
    "                    text = nl.Text(tokens)\n",
    "                    token_words=[word for word in tokens if word.isalpha()]\n",
    "                    stopword=stopwords.words('english')\n",
    "                    stopword.append('dont')\n",
    "                    stopword.append('didnt')\n",
    "                    stopword.append('doesnt')\n",
    "                    stopword.append('cant')\n",
    "                    stopword.append('couldnt')\n",
    "                    stopword.append('couldve')\n",
    "                    stopword.append('im')\n",
    "                    stopword.append('ive')\n",
    "                    stopword.append('isnt')\n",
    "                    stopword.append('theres')\n",
    "                    stopword.append('wasnt')\n",
    "                    stopword.append('wouldnt')\n",
    "                    stopword.append('a')\n",
    "                    stopword.append('also')\n",
    "                    token_words=[w for w in token_words if not w in stopword]\n",
    "                    porter =PorterStemmer()\n",
    "                    token_stemmed=[porter.stem(w) for w in token_words]\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    #creating worlcloud\n",
    "                    #for w in token_words:\n",
    "                        #if type(w)!='str':\n",
    "                            #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "\n",
    "                    cloudstring=(\" \").join(token_words)\n",
    "                    wordcloud = WordCloud(max_font_size=50,max_words=100, background_color=\"black\").generate(cloudstring)\n",
    "                    plt.figure(figsize=(20,20))\n",
    "                    ax = plt.subplot(221)\n",
    "                    # plot wordcloud in matplotlib\n",
    "                    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(\"WordCloud\")\n",
    "                    plt.grid(True)\n",
    "                    #plotting bi-gram cloud\n",
    "                    # setup and score the bigrams using the raw frequency.\n",
    "                    finder = BigramCollocationFinder.from_words(token_words)\n",
    "                    bigram_measures = BigramAssocMeasures()\n",
    "                    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "                    scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    "                    word_dict = {} \n",
    "                    listLen = len(scoredList)\n",
    "                    for i in range(listLen):\n",
    "                        word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "                    wordCloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\")\n",
    "                    plt.subplot(222) \n",
    "                    wordCloud.generate_from_frequencies(word_dict)\n",
    "\n",
    "                    plt.title('Most frequently occurring bigrams connected with an underscore_')\n",
    "                    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "\n",
    "                    #plotting frequency distribution\n",
    "                    plt.figure(figsize=(25,5))\n",
    "                    ax = plt.subplot(121)\n",
    "                    freqdist = nl.FreqDist(token_words)\n",
    "                    plt.subplot(121) \n",
    "                    plt.title(\"Frequency Distribution of top 50 token words\")\n",
    "                    freqdist.plot(50)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                self.wc_drop.observe(wc_change,'value')\n",
    "        except Exception as ve:\n",
    "            print(ve)\n",
    "            print(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "            def wc_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Visualise Text through Word Clouds. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.wc_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.wc_drop)\n",
    "                    #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "                    #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                        #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    " \n",
    "\n",
    "                    all_reviews=self.org_df[self.wc_drop.value]\n",
    "                    #tokenization,removing stopwords, punctuation and stemming\n",
    "                    all_ans=\"\"\n",
    "                    for review in all_reviews:\n",
    "                        all_ans=all_ans+review+\"\\n\"\n",
    "                    all_ans= all_ans.replace(\"'\", \"\")\n",
    "                    tokens=word_tokenize(all_ans)\n",
    "                    tokens=[w.lower() for w in tokens]\n",
    "                    text = nl.Text(tokens)\n",
    "                    token_words=[word for word in tokens if word.isalpha()]\n",
    "                    stopword=stopwords.words('english')\n",
    "                    stopword.append('dont')\n",
    "                    stopword.append('didnt')\n",
    "                    stopword.append('doesnt')\n",
    "                    stopword.append('cant')\n",
    "                    stopword.append('couldnt')\n",
    "                    stopword.append('couldve')\n",
    "                    stopword.append('im')\n",
    "                    stopword.append('ive')\n",
    "                    stopword.append('isnt')\n",
    "                    stopword.append('theres')\n",
    "                    stopword.append('wasnt')\n",
    "                    stopword.append('wouldnt')\n",
    "                    stopword.append('a')\n",
    "                    stopword.append('also')\n",
    "                    token_words=[w for w in token_words if not w in stopword]\n",
    "                    porter =PorterStemmer()\n",
    "                    token_stemmed=[porter.stem(w) for w in token_words]\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    #creating worlcloud\n",
    "                    #for w in token_words:\n",
    "                        #if type(w)!='str':\n",
    "                            #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "\n",
    "                    cloudstring=(\" \").join(token_words)\n",
    "                    wordcloud = WordCloud(max_font_size=50,max_words=100, background_color=\"black\").generate(cloudstring)\n",
    "                    plt.figure(figsize=(20,20))\n",
    "                    ax = plt.subplot(221)\n",
    "                    # plot wordcloud in matplotlib\n",
    "                    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.title(\"WordCloud\")\n",
    "                    plt.grid(True)\n",
    "                    #plotting bi-gram cloud\n",
    "                    # setup and score the bigrams using the raw frequency.\n",
    "                    finder = BigramCollocationFinder.from_words(token_words)\n",
    "                    bigram_measures = BigramAssocMeasures()\n",
    "                    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "                    scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    "                    word_dict = {} \n",
    "                    listLen = len(scoredList)\n",
    "                    for i in range(listLen):\n",
    "                        word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "                    wordCloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\")\n",
    "                    plt.subplot(222) \n",
    "                    wordCloud.generate_from_frequencies(word_dict)\n",
    "\n",
    "                    plt.title('Most frequently occurring bigrams connected with an underscore_')\n",
    "                    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "\n",
    "                    #plotting frequency distribution\n",
    "                    plt.figure(figsize=(25,5))\n",
    "                    ax = plt.subplot(121)\n",
    "                    freqdist = nl.FreqDist(token_words)\n",
    "                    plt.subplot(121) \n",
    "                    plt.title(\"Frequency Distribution of top 50 token words\")\n",
    "                    freqdist.plot(50)\n",
    "\n",
    "        finally:\n",
    "            self.wc_drop.observe(wc_change,'value')\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "            \n",
    "    def tm_data(self,b):\n",
    "        try:\n",
    "            if self.sheet==None or self.sheet==\"\":\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(\"Please change data settings before use\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Explore topics through topic modelling. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                self.tm_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.tm_drop)\n",
    "                #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "\n",
    "                #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                    #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "                data=self.org_df[self.tm_drop.value]\n",
    "\n",
    "                vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                                     stop_words='english', lowercase=True, \n",
    "                                     token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "                data_vectorized = vectorizer.fit_transform(data)\n",
    "                # Build a Latent Dirichlet Allocation Model\n",
    "                lda_model = LatentDirichletAllocation(n_components=10, max_iter=10, learning_method='online')\n",
    "                lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))\n",
    "                # Visualize the topics\n",
    "                pyLDAvis.enable_notebook()\n",
    "                panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "                display(panel)\n",
    "\n",
    "\n",
    "                def tm_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Explore topics through topic modelling. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                    display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.tm_drop=widgets.Dropdown(options=self.cols,value=self.cols[4],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.tm_drop)\n",
    "                    #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "\n",
    "                    #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                        #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "                    data=self.org_df[self.tm_drop.value]\n",
    "\n",
    "                    vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                                         stop_words='english', lowercase=True, \n",
    "                                         token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "                    data_vectorized = vectorizer.fit_transform(data)\n",
    "                    # Build a Latent Dirichlet Allocation Model\n",
    "                    lda_model = LatentDirichletAllocation(n_components=10, max_iter=10, learning_method='online')\n",
    "                    lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    # Visualize the topics\n",
    "                    pyLDAvis.enable_notebook()\n",
    "                    panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "                    display(panel)\n",
    "\n",
    "                self.tm_drop.observe(tm_change,'value')\n",
    "        except Exception as ve:\n",
    "            print(ve)\n",
    "            print(\"That is not a suitable feature for Topic Modelling. Select a feature with text values only.\")\n",
    "            def tm_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Explore topics through topic modelling. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                    display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.tm_drop=widgets.Dropdown(options=self.cols,value=self.cols[4],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.tm_drop)\n",
    "                    #print(self.org_df[self.wc_drop.value].dtype)\n",
    "\n",
    "\n",
    "                    #if is_string_dtype(self.org_df[self.wc_drop.value])==False:\n",
    "                        #raise ValueError(\"That is not a suitable feature to generate WordCloud. Select a feature with text values only.\")\n",
    "                    data=self.org_df[self.tm_drop.value]\n",
    "\n",
    "                    vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                                         stop_words='english', lowercase=True, \n",
    "                                         token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "                    data_vectorized = vectorizer.fit_transform(data)\n",
    "                    # Build a Latent Dirichlet Allocation Model\n",
    "                    lda_model = LatentDirichletAllocation(n_components=10, max_iter=10, learning_method='online')\n",
    "                    lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    # Visualize the topics\n",
    "                    pyLDAvis.enable_notebook()\n",
    "                    panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "                    display(panel)\n",
    "\n",
    "            \n",
    "        finally:\n",
    "            self.tm_drop.observe(tm_change,'value')\n",
    "            \n",
    "\n",
    "    def sa_data(self,b):\n",
    "        try:\n",
    "            if self.sheet==None or self.sheet==\"\":\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(\"Please change data settings before use\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Opinion mining. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                self.sa_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.sa_drop)\n",
    "                \n",
    "                all_reviews=self.org_df[self.sa_drop.value]\n",
    "                #sentiment analysis\n",
    "                analyser = SentimentIntensityAnalyzer()\n",
    "                \n",
    "                pos_entire=[]\n",
    "                neg_entire=[]\n",
    "                neutral_entire=[]\n",
    "\n",
    "                \n",
    "                for review in all_reviews:\n",
    "                    scores = analyser.polarity_scores(review)\n",
    "                    if scores['compound']<=-0.5:\n",
    "                        neg_entire.append(review)\n",
    "                    if scores['compound']>=0.5:\n",
    "                        pos_entire.append(review)\n",
    "                    if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                        neutral_entire.append(review)\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))        \n",
    "                type_length=[len(pos_entire),len(neutral_entire),len(neg_entire)]\n",
    "                sent_type=['positive','neutral','negative']\n",
    "                plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "                plt.title('Sentiment distribution')\n",
    "                plt.show()\n",
    "\n",
    "                bars1 = [len(pos_entire)]\n",
    "                bars2 = [len(neutral_entire)]\n",
    "                bars3 = [len(neg_entire)]\n",
    "                sent_type=['positive','neutral','negative']\n",
    "\n",
    "                # set width of bar\n",
    "                barWidth = 0.25\n",
    "                # Set position of bar on X axis\n",
    "                r1 = np.arange(len(bars1))\n",
    "                r2 = [x + barWidth for x in r1]\n",
    "                r3 = [x + barWidth for x in r2]\n",
    "\n",
    "                # Make the plot\n",
    "                plt.figure(figsize=(15,10))\n",
    "                plt.bar(r1, bars1, width=barWidth, edgecolor='white', label='Positive')\n",
    "                plt.bar(r2, bars2, width=barWidth, edgecolor='white', label='Neutral')\n",
    "                plt.bar(r3, bars3, width=barWidth, edgecolor='white', label='Negative')\n",
    "\n",
    "                # Add xticks on the middle of the group bars\n",
    "                plt.xlabel('Group', fontweight='bold')\n",
    "                plt.xticks([r + barWidth for r in range(len(bars1))], ['Entire Cohort'])\n",
    "\n",
    "\n",
    "                # Create legend & Show graphic\n",
    "                plt.legend()\n",
    "                plt.title('Sentiment Distributions')\n",
    "                plt.show()\n",
    "                \n",
    "                def sa_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Opinion mining. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                    display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.sa_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.sa_drop)\n",
    "\n",
    "                    all_reviews=self.org_df[self.sa_drop.value]\n",
    "                    #sentiment analysis\n",
    "                    analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "                    pos_entire=[]\n",
    "                    neg_entire=[]\n",
    "                    neutral_entire=[]\n",
    "\n",
    "\n",
    "                    for review in all_reviews:\n",
    "                        scores = analyser.polarity_scores(review)\n",
    "                        if scores['compound']<=-0.5:\n",
    "                            neg_entire.append(review)\n",
    "                        if scores['compound']>=0.5:\n",
    "                            pos_entire.append(review)\n",
    "                        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                            neutral_entire.append(review)\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))        \n",
    "                    type_length=[len(pos_entire),len(neutral_entire),len(neg_entire)]\n",
    "                    sent_type=['positive','neutral','negative']\n",
    "                    plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "                    plt.title('Sentiment distribution')\n",
    "                    plt.show()\n",
    "\n",
    "                    bars1 = [len(pos_entire)]\n",
    "                    bars2 = [len(neutral_entire)]\n",
    "                    bars3 = [len(neg_entire)]\n",
    "                    sent_type=['positive','neutral','negative']\n",
    "\n",
    "                    # set width of bar\n",
    "                    barWidth = 0.25\n",
    "                    # Set position of bar on X axis\n",
    "                    r1 = np.arange(len(bars1))\n",
    "                    r2 = [x + barWidth for x in r1]\n",
    "                    r3 = [x + barWidth for x in r2]\n",
    "\n",
    "                    # Make the plot\n",
    "                    plt.figure(figsize=(15,10))\n",
    "                    plt.bar(r1, bars1, width=barWidth, edgecolor='white', label='Positive')\n",
    "                    plt.bar(r2, bars2, width=barWidth, edgecolor='white', label='Neutral')\n",
    "                    plt.bar(r3, bars3, width=barWidth, edgecolor='white', label='Negative')\n",
    "\n",
    "                    # Add xticks on the middle of the group bars\n",
    "                    plt.xlabel('Group', fontweight='bold')\n",
    "                    plt.xticks([r + barWidth for r in range(len(bars1))], ['Entire Cohort'])\n",
    "\n",
    "\n",
    "                    # Create legend & Show graphic\n",
    "                    plt.legend()\n",
    "                    plt.title('Sentiment Distributions')\n",
    "                    plt.show()\n",
    "                    \n",
    "                \n",
    "                self.sa_drop.observe(sa_change,'value')\n",
    "                \n",
    "        except Exception as ve:\n",
    "            print(ve)\n",
    "            print(\"That is not a suitable feature for Sentiment Analysis. Select a feature with text values only.\")\n",
    "            def sa_change(change):\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Opinion mining. Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                #self.sa_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.sa_drop)\n",
    "                \n",
    "                all_reviews=self.org_df[self.sa_drop.value]\n",
    "                #sentiment analysis\n",
    "                analyser = SentimentIntensityAnalyzer()\n",
    "                \n",
    "                pos_entire=[]\n",
    "                neg_entire=[]\n",
    "                neutral_entire=[]\n",
    "\n",
    "                \n",
    "                for review in all_reviews:\n",
    "                    scores = analyser.polarity_scores(review)\n",
    "                    if scores['compound']<=-0.5:\n",
    "                        neg_entire.append(review)\n",
    "                    if scores['compound']>=0.5:\n",
    "                        pos_entire.append(review)\n",
    "                    if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                        neutral_entire.append(review)\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))        \n",
    "                type_length=[len(pos_entire),len(neutral_entire),len(neg_entire)]\n",
    "                sent_type=['positive','neutral','negative']\n",
    "                plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "                plt.title('Sentiment distribution')\n",
    "                plt.show()\n",
    "\n",
    "                bars1 = [len(pos_entire)]\n",
    "                bars2 = [len(neutral_entire)]\n",
    "                bars3 = [len(neg_entire)]\n",
    "                sent_type=['positive','neutral','negative']\n",
    "\n",
    "                # set width of bar\n",
    "                barWidth = 0.25\n",
    "                # Set position of bar on X axis\n",
    "                r1 = np.arange(len(bars1))\n",
    "                r2 = [x + barWidth for x in r1]\n",
    "                r3 = [x + barWidth for x in r2]\n",
    "\n",
    "                # Make the plot\n",
    "                plt.figure(figsize=(15,10))\n",
    "                plt.bar(r1, bars1, width=barWidth, edgecolor='white', label='Positive')\n",
    "                plt.bar(r2, bars2, width=barWidth, edgecolor='white', label='Neutral')\n",
    "                plt.bar(r3, bars3, width=barWidth, edgecolor='white', label='Negative')\n",
    "\n",
    "                # Add xticks on the middle of the group bars\n",
    "                plt.xlabel('Group', fontweight='bold')\n",
    "                plt.xticks([r + barWidth for r in range(len(bars1))], ['Entire Cohort'])\n",
    "\n",
    "\n",
    "                # Create legend & Show graphic\n",
    "                plt.legend()\n",
    "                plt.title('Sentiment Distributions')\n",
    "                plt.show()\n",
    "                \n",
    "            \n",
    "        finally:\n",
    "            self.sa_drop.observe(sa_change,'value')\n",
    "            \n",
    "                \n",
    "        \n",
    "\n",
    "    def ts_data(self,b):\n",
    "        try:\n",
    "            if self.sheet==None or self.sheet==\"\":\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(\"Please change data settings before use\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                self.ts_drop=widgets.Dropdown(options=self.cols,value=self.cols[0],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.ts_drop)\n",
    "                \n",
    "                all_reviews=self.org_df[self.ts_drop.value]\n",
    "                \n",
    "                \n",
    "                pos_entire=[]\n",
    "                neg_entire=[]\n",
    "                neutral_entire=[]\n",
    "                analyser = SentimentIntensityAnalyzer()\n",
    "                for review in all_reviews:\n",
    "                    scores = analyser.polarity_scores(review)\n",
    "                    if scores['compound']<=-0.5:\n",
    "                        neg_entire.append(review)\n",
    "                    if scores['compound']>=0.5:\n",
    "                        pos_entire.append(review)\n",
    "                    if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                        neutral_entire.append(review)\n",
    "                #sentiment analysis\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))\n",
    "                self.bpos = widgets.Button(description=\"Next positive text\",button_style='success',layout=Layout(width='175px'))\n",
    "                self.bneg = widgets.Button(description=\"Next negative text\",button_style='danger',layout=Layout(width='175px'))\n",
    "                self.bneu = widgets.Button(description=\"Next neutral text\",button_style='info',layout=Layout(width='175px'))\n",
    "                display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                \n",
    "                ########################################################################\n",
    "                \n",
    "                \n",
    "                def posref(b):\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    #display(HBox([bpos,bneu,bneg]))\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nPositive Text\\n\")\n",
    "                    display(pos_entire[0])\n",
    "                    text_for_summ=pos_entire[0]\n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "                    #print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "                    item_rem=pos_entire.pop(0)\n",
    "                    pos_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bpos.on_click(posref)\n",
    "\n",
    "                def neuref(b):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nNeutral Text\\n\")\n",
    "                    display(neutral_entire[0])\n",
    "                    text_for_summ=neutral_entire[0]\n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(summary) \n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    item_rem=neutral_entire.pop(0)\n",
    "                    neutral_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bneu.on_click(neuref)\n",
    "\n",
    "                def negref(b):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nNegative Text\\n\")\n",
    "                    display(neg_entire[0])\n",
    "\n",
    "                    text_for_summ=neg_entire[0]        \n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(summary) \n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    item_rem=neg_entire.pop(0)\n",
    "                    neg_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bneg.on_click(negref)\n",
    "                \n",
    "                \n",
    "                def ts_change(change):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(widgets.Label(value=\"Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                    display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                    #self.ts_drop=widgets.Dropdown(options=self.cols,value=self.cols[4],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                    display(self.ts_drop)\n",
    "\n",
    "                    all_reviews=self.org_df[self.ts_drop.value]\n",
    "\n",
    "\n",
    "                    pos_entire=[]\n",
    "                    neg_entire=[]\n",
    "                    neutral_entire=[]\n",
    "                    analyser = SentimentIntensityAnalyzer()\n",
    "                    for review in all_reviews:\n",
    "                        scores = analyser.polarity_scores(review)\n",
    "                        if scores['compound']<=-0.5:\n",
    "                            neg_entire.append(review)\n",
    "                        if scores['compound']>=0.5:\n",
    "                            pos_entire.append(review)\n",
    "                        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                            neutral_entire.append(review)\n",
    "                    #sentiment analysis\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    self.bpos = widgets.Button(description=\"Next positive text\",button_style='success',layout=Layout(width='175px'))\n",
    "                    self.bneg = widgets.Button(description=\"Next negative text\",button_style='danger',layout=Layout(width='175px'))\n",
    "                    self.bneu = widgets.Button(description=\"Next neutral text\",button_style='info',layout=Layout(width='175px'))\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "\n",
    "                    ########################################################################\n",
    "\n",
    "\n",
    "                    def posref(b):\n",
    "                        #clear_output()\n",
    "                        #display(HBox([item for item in items]))\n",
    "                        #display(HBox([bpos,bneu,bneg]))\n",
    "                        clear_output()\n",
    "                        display(HBox([item for item in self.items]))\n",
    "                        display(self.import_data)\n",
    "                        display(self.ts_drop)\n",
    "                        display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                        print(\"\\nPositive Text\\n\")\n",
    "                        display(pos_entire[0])\n",
    "                        text_for_summ=pos_entire[0]\n",
    "                        article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                        article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                        formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                        sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                        stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                        word_frequencies = {}  \n",
    "                        for word in nl.word_tokenize(formatted_article_text):  \n",
    "                            if word not in stopwords:\n",
    "                                if word not in word_frequencies.keys():\n",
    "                                    word_frequencies[word] = 1\n",
    "                                else:\n",
    "                                    word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                        maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                        for word in word_frequencies.keys():  \n",
    "                            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                        sentence_scores = {}  \n",
    "                        for sent in sentence_list:  \n",
    "                            for word in nl.word_tokenize(sent.lower()):\n",
    "                                if word in word_frequencies.keys():\n",
    "                                    if len(sent.split(' ')) < 30:\n",
    "                                        if sent not in sentence_scores.keys():\n",
    "                                            sentence_scores[sent] = word_frequencies[word]\n",
    "                                        else:\n",
    "                                            sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                        import heapq  \n",
    "                        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                        summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                        tokens = word_tokenize(summary)\n",
    "                        tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                        visualizer = PosTagVisualizer()\n",
    "                        visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "                        #print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        display('===================================SUMMARY=========================================================')\n",
    "                        #display(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                        print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                        print('\\n')\n",
    "\n",
    "\n",
    "                        item_rem=pos_entire.pop(0)\n",
    "                        pos_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                    self.bpos.on_click(posref)\n",
    "\n",
    "                    def neuref(b):\n",
    "                        clear_output()\n",
    "                        display(HBox([item for item in self.items]))\n",
    "                        display(self.import_data)\n",
    "                        display(self.ts_drop)\n",
    "                        display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                        print(\"\\nNeutral Text\\n\")\n",
    "                        display(neutral_entire[0])\n",
    "                        text_for_summ=neutral_entire[0]\n",
    "                        article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                        article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                        formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                        sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                        stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                        word_frequencies = {}  \n",
    "                        for word in nl.word_tokenize(formatted_article_text):  \n",
    "                            if word not in stopwords:\n",
    "                                if word not in word_frequencies.keys():\n",
    "                                    word_frequencies[word] = 1\n",
    "                                else:\n",
    "                                    word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                        maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                        for word in word_frequencies.keys():  \n",
    "                            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                        sentence_scores = {}  \n",
    "                        for sent in sentence_list:  \n",
    "                            for word in nl.word_tokenize(sent.lower()):\n",
    "                                if word in word_frequencies.keys():\n",
    "                                    if len(sent.split(' ')) < 30:\n",
    "                                        if sent not in sentence_scores.keys():\n",
    "                                            sentence_scores[sent] = word_frequencies[word]\n",
    "                                        else:\n",
    "                                            sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                        import heapq  \n",
    "                        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                        summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                        tokens = word_tokenize(summary)\n",
    "                        tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                        visualizer = PosTagVisualizer()\n",
    "                        visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        display('===================================SUMMARY=========================================================')\n",
    "                        #display(summary) \n",
    "                        print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        item_rem=neutral_entire.pop(0)\n",
    "                        neutral_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                    self.bneu.on_click(neuref)\n",
    "\n",
    "                    def negref(b):\n",
    "                        clear_output()\n",
    "                        display(HBox([item for item in self.items]))\n",
    "                        display(self.import_data)\n",
    "                        display(self.ts_drop)\n",
    "                        display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                        print(\"\\nNegative Text\\n\")\n",
    "                        display(neg_entire[0])\n",
    "\n",
    "                        text_for_summ=neg_entire[0]        \n",
    "                        article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                        article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                        formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                        sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                        stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                        word_frequencies = {}  \n",
    "                        for word in nl.word_tokenize(formatted_article_text):  \n",
    "                            if word not in stopwords:\n",
    "                                if word not in word_frequencies.keys():\n",
    "                                    word_frequencies[word] = 1\n",
    "                                else:\n",
    "                                    word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                        maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                        for word in word_frequencies.keys():  \n",
    "                            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                        sentence_scores = {}  \n",
    "                        for sent in sentence_list:  \n",
    "                            for word in nl.word_tokenize(sent.lower()):\n",
    "                                if word in word_frequencies.keys():\n",
    "                                    if len(sent.split(' ')) < 30:\n",
    "                                        if sent not in sentence_scores.keys():\n",
    "                                            sentence_scores[sent] = word_frequencies[word]\n",
    "                                        else:\n",
    "                                            sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                        import heapq  \n",
    "                        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                        summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                        tokens = word_tokenize(summary)\n",
    "                        tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                        visualizer = PosTagVisualizer()\n",
    "                        visualizer.transform(tagged)\n",
    "\n",
    "                        display('===================================SUMMARY=========================================================')\n",
    "                        #display(summary) \n",
    "                        print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        item_rem=neg_entire.pop(0)\n",
    "                        neg_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                    self.bneg.on_click(negref)\n",
    "                    self.ts_drop.observe(ts_change,'value')\n",
    "                \n",
    "        except Exception as ve:\n",
    "            print(ve)\n",
    "            print(\"That is not a suitable feature for Sentiment Analysis. Select a feature with text values only.\")\n",
    "            def ts_change(change):\n",
    "                clear_output()\n",
    "                display(HBox([item for item in self.items]))\n",
    "                display(self.import_data)\n",
    "                display(widgets.Label(value=\"Select a feature with text values only.\",layout=Layout(width='50%')))\n",
    "                display(widgets.Label(value=\"Please be patient while the output is generated. This may take a few moments.\",layout=Layout(width='50%')))\n",
    "\n",
    "\n",
    "                #self.ts_drop=widgets.Dropdown(options=self.cols,value=self.cols[4],layout=Layout(width='50%'),description='Feature:',tooltip='Select a feature to generate wordcloud',disabled=False,)\n",
    "                display(self.ts_drop)\n",
    "                \n",
    "                all_reviews=self.org_df[self.ts_drop.value]\n",
    "                \n",
    "                \n",
    "                pos_entire=[]\n",
    "                neg_entire=[]\n",
    "                neutral_entire=[]\n",
    "                analyser = SentimentIntensityAnalyzer()\n",
    "                for review in all_reviews:\n",
    "                    scores = analyser.polarity_scores(review)\n",
    "                    if scores['compound']<=-0.5:\n",
    "                        neg_entire.append(review)\n",
    "                    if scores['compound']>=0.5:\n",
    "                        pos_entire.append(review)\n",
    "                    if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "                        neutral_entire.append(review)\n",
    "                #sentiment analysis\n",
    "                #clear_output()\n",
    "                #display(HBox([item for item in items]))\n",
    "                self.bpos = widgets.Button(description=\"Next positive text\",button_style='success',layout=Layout(width='175px'))\n",
    "                self.bneg = widgets.Button(description=\"Next negative text\",button_style='danger',layout=Layout(width='175px'))\n",
    "                self.bneu = widgets.Button(description=\"Next neutral text\",button_style='info',layout=Layout(width='175px'))\n",
    "                display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                \n",
    "                ########################################################################\n",
    "                \n",
    "                \n",
    "                def posref(b):\n",
    "                    #clear_output()\n",
    "                    #display(HBox([item for item in items]))\n",
    "                    #display(HBox([bpos,bneu,bneg]))\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nPositive Text\\n\")\n",
    "                    display(pos_entire[0])\n",
    "                    text_for_summ=pos_entire[0]\n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "                    #print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "                    item_rem=pos_entire.pop(0)\n",
    "                    pos_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bpos.on_click(posref)\n",
    "\n",
    "                def neuref(b):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nNeutral Text\\n\")\n",
    "                    display(neutral_entire[0])\n",
    "                    text_for_summ=neutral_entire[0]\n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(summary) \n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    item_rem=neutral_entire.pop(0)\n",
    "                    neutral_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bneu.on_click(neuref)\n",
    "\n",
    "                def negref(b):\n",
    "                    clear_output()\n",
    "                    display(HBox([item for item in self.items]))\n",
    "                    display(self.import_data)\n",
    "                    display(self.ts_drop)\n",
    "                    display(HBox([self.bpos,self.bneu,self.bneg]))\n",
    "                    print(\"\\nNegative Text\\n\")\n",
    "                    display(neg_entire[0])\n",
    "\n",
    "                    text_for_summ=neg_entire[0]        \n",
    "                    article_text = re.sub(r'\\[[0-9]*\\]', ' ', text_for_summ)  \n",
    "                    article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "                    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "                    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "                    sentence_list = nl.sent_tokenize(article_text)  \n",
    "\n",
    "                    stopwords = nl.corpus.stopwords.words('english')\n",
    "\n",
    "                    word_frequencies = {}  \n",
    "                    for word in nl.word_tokenize(formatted_article_text):  \n",
    "                        if word not in stopwords:\n",
    "                            if word not in word_frequencies.keys():\n",
    "                                word_frequencies[word] = 1\n",
    "                            else:\n",
    "                                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "                    for word in word_frequencies.keys():  \n",
    "                        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "                    sentence_scores = {}  \n",
    "                    for sent in sentence_list:  \n",
    "                        for word in nl.word_tokenize(sent.lower()):\n",
    "                            if word in word_frequencies.keys():\n",
    "                                if len(sent.split(' ')) < 30:\n",
    "                                    if sent not in sentence_scores.keys():\n",
    "                                        sentence_scores[sent] = word_frequencies[word]\n",
    "                                    else:\n",
    "                                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "                    import heapq  \n",
    "                    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "                    summary = ' '.join(summary_sentences) \n",
    "\n",
    "\n",
    "                    tokens = word_tokenize(summary)\n",
    "                    tagged = pos_tag(tokens)\n",
    "\n",
    "\n",
    "                    visualizer = PosTagVisualizer()\n",
    "                    visualizer.transform(tagged)\n",
    "\n",
    "                    display('===================================SUMMARY=========================================================')\n",
    "                    #display(summary) \n",
    "                    print(' '.join((visualizer.colorize(token, color) for color, token in visualizer.tagged)))\n",
    "                    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    item_rem=neg_entire.pop(0)\n",
    "                    neg_entire.append(item_rem)\n",
    "\n",
    "\n",
    "\n",
    "                self.bneg.on_click(negref)\n",
    "                \n",
    "        finally:\n",
    "            self.ts_drop.observe(ts_change,'value')\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    obj=SSP()\n",
    "    '''\n",
    "    def new_data(b):\n",
    "        from tkinter import filedialog\n",
    "\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "\n",
    "        file_path = filedialog.askopenfilename()\n",
    "        labelf=widgets.Label(value=\"Selected Data : \"+str(file_path))\n",
    "        display(labelf)\n",
    "        return file_path\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    import_data=widgets.Button(\n",
    "    description='Import New Data',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to import new data',\n",
    "    icon='check'\n",
    "    )\n",
    "    \n",
    "    file_path=import_data.on_click(new_data)\n",
    "    display(import_data)\n",
    "    items[0].on_click(preview_data)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d706b03cab4799aa7e21a261bc2746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Data Settings', layout=Layout(width='175px'), style=Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa933d7223c64a159e77e476c0aef278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Import New Data', icon='check', style=ButtonStyle(), tooltip='Clic…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292eafc31bb248fe99c8414c2944ce1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', index=4, layout=Layout(width='50%'), options=('Student ID', 'Gender', 'Intern…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42afa278a1f0431599a44c35768cd3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Next positive text', layout=Layout(width='175px'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negative Text\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Since the first day back, the tutors and project leaders have all been constantly repeating that we should learn how to manage our own learning. In the beginning, it felt like a nagging at the back of my head, because yeah, of course we have to manage our own learning! Were university students! In fact, I remember rating myself quite high in the commitment and managing time sections in the first template.Now that 3 weeks have passed, Im starting to appreciate the constant reminders more. The routine of going in and out of class makes it so easy to become lax, even more so now that Im more comfortable with my team members. I really want to improve from last semester, as I only got a 5 for ENGG1100. While I do complete all my tasks by the due date, Im worried that Ill procrastinate too much and end up leaving all my work to the last minute yet again. That wouldnt be the best, particularly for ENGG1200 where our project is basically the accumulation of the teams work over the whole semester. Also,whenever I feel overwhelmed, I have the tendency to avoid tasks, which only makes me feel more anxious than I was to begin with!But its only week 3, I can still pick up the pace so I wont fall behind the rest of the semester. Since I do know my weaknesses and tendencies while studying, now I have to come up with the proper study approach. One of my goals is to be more organised. A fellow team member introduced me to a really cool time management app that will allow me to keep track of all my due dates, tasks and timetable all at once, and Im really grateful for it. I also plan to keep a logbook, so that Ill have a physical method of documentation. Keeping one last semester for ENGG1100 really helped me stay on top of things and Im hoping that will be the case for this semester as well.Other than staying organised, I guess one more thing I should work on is to stop being too hard on myself. Ill have to keep reminding myself to just keep trying my best no matter the outcome so that I dont turn into a time-ticking stress bomb! For now, Ill just keep working towards these two goals. Ill be looking forward to see how much Ive improved at the end of the semester.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'===================================SUMMARY========================================================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mIll\u001b[0m \u001b[0;34mhave\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34mkeep\u001b[0m \u001b[0;34mreminding\u001b[0m \u001b[0;0mmyself\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;36mjust\u001b[0m \u001b[0;34mkeep\u001b[0m \u001b[0;34mtrying\u001b[0m \u001b[0;35mmy\u001b[0m \u001b[0;31mbest\u001b[0m \u001b[0;30mno\u001b[0m \u001b[0;32mmatter\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32moutcome\u001b[0m \u001b[1;37mso\u001b[0m \u001b[1;37mthat\u001b[0m \u001b[0;0mI\u001b[0m \u001b[0;34mdont\u001b[0m \u001b[0;34mturn\u001b[0m \u001b[1;37minto\u001b[0m \u001b[0;30ma\u001b[0m \u001b[0;31mtime-ticking\u001b[0m \u001b[0;32mstress\u001b[0m \u001b[0;32mbomb\u001b[0m \u001b[0;0m!\u001b[0m \u001b[0;30mThat\u001b[0m \u001b[0;32mwouldnt\u001b[0m \u001b[0;34mbe\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;31mbest\u001b[0m \u001b[0;0m,\u001b[0m \u001b[0;36mparticularly\u001b[0m \u001b[1;37mfor\u001b[0m \u001b[0;32mENGG1200\u001b[0m \u001b[0;30mwhere\u001b[0m \u001b[0;35mour\u001b[0m \u001b[0;32mproject\u001b[0m \u001b[0;34mis\u001b[0m \u001b[0;36mbasically\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32maccumulation\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32mteams\u001b[0m \u001b[0;34mwork\u001b[0m \u001b[1;37mover\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;31mwhole\u001b[0m \u001b[0;32msemester\u001b[0m \u001b[0;0m.\u001b[0m \u001b[1;37mSince\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;31mfirst\u001b[0m \u001b[0;32mday\u001b[0m \u001b[0;36mback\u001b[0m \u001b[0;0m,\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32mtutors\u001b[0m \u001b[0;30mand\u001b[0m \u001b[0;32mproject\u001b[0m \u001b[0;32mleaders\u001b[0m \u001b[0;34mhave\u001b[0m \u001b[0;30mall\u001b[0m \u001b[0;34mbeen\u001b[0m \u001b[0;36mconstantly\u001b[0m \u001b[0;34mrepeating\u001b[0m \u001b[1;37mthat\u001b[0m \u001b[0;0mwe\u001b[0m \u001b[0;33mshould\u001b[0m \u001b[0;34mlearn\u001b[0m \u001b[0;30mhow\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34mmanage\u001b[0m \u001b[0;35mour\u001b[0m \u001b[0;31mown\u001b[0m \u001b[0;32mlearning\u001b[0m \u001b[0;0m.\u001b[0m \u001b[0;0mI\u001b[0m \u001b[0;36mreally\u001b[0m \u001b[0;34mwant\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34mimprove\u001b[0m \u001b[1;37mfrom\u001b[0m \u001b[0;31mlast\u001b[0m \u001b[0;32msemester\u001b[0m \u001b[0;0m,\u001b[0m \u001b[1;37mas\u001b[0m \u001b[0;0mI\u001b[0m \u001b[0;36monly\u001b[0m \u001b[0;34mgot\u001b[0m \u001b[0;30ma\u001b[0m \u001b[0;30m5\u001b[0m \u001b[1;37mfor\u001b[0m \u001b[0;32mENGG1100\u001b[0m \u001b[0;0m.\u001b[0m \u001b[0;32mIll\u001b[0m \u001b[0;34mbe\u001b[0m \u001b[0;34mlooking\u001b[0m \u001b[0;36mforward\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34msee\u001b[0m \u001b[0;30mhow\u001b[0m \u001b[0;31mmuch\u001b[0m \u001b[0;32mIve\u001b[0m \u001b[0;34mimproved\u001b[0m \u001b[1;37mat\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32mend\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32msemester\u001b[0m \u001b[0;0m.\u001b[0m \u001b[1;37mIn\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32mbeginning\u001b[0m \u001b[0;0m,\u001b[0m \u001b[0;0mit\u001b[0m \u001b[0;34mfelt\u001b[0m \u001b[1;37mlike\u001b[0m \u001b[0;30ma\u001b[0m \u001b[0;32mnagging\u001b[0m \u001b[1;37mat\u001b[0m \u001b[0;30mthe\u001b[0m \u001b[0;32mback\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;35mmy\u001b[0m \u001b[0;32mhead\u001b[0m \u001b[0;0m,\u001b[0m \u001b[1;37mbecause\u001b[0m \u001b[0;0myeah\u001b[0m \u001b[0;0m,\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;32mcourse\u001b[0m \u001b[0;0mwe\u001b[0m \u001b[0;34mhave\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34mmanage\u001b[0m \u001b[0;35mour\u001b[0m \u001b[0;31mown\u001b[0m \u001b[0;32mlearning\u001b[0m \u001b[0;0m!\u001b[0m \u001b[0;30mThe\u001b[0m \u001b[0;32mroutine\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;34mgoing\u001b[0m \u001b[1;37min\u001b[0m \u001b[0;30mand\u001b[0m \u001b[1;37mout\u001b[0m \u001b[1;37mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;34mmakes\u001b[0m \u001b[0;0mit\u001b[0m \u001b[0;36mso\u001b[0m \u001b[0;31measy\u001b[0m \u001b[0;33mto\u001b[0m \u001b[0;34mbecome\u001b[0m \u001b[0;31mlax\u001b[0m \u001b[0;0m,\u001b[0m \u001b[0;36meven\u001b[0m \u001b[0;36mmore\u001b[0m \u001b[0;36mso\u001b[0m \u001b[0;36mnow\u001b[0m \u001b[1;37mthat\u001b[0m \u001b[0;32mIm\u001b[0m \u001b[0;36mmore\u001b[0m \u001b[0;31mcomfortable\u001b[0m \u001b[1;37mwith\u001b[0m \u001b[0;35mmy\u001b[0m \u001b[0;32mteam\u001b[0m \u001b[0;32mmembers\u001b[0m \u001b[0;0m.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    np.warnings.filterwarnings('ignore')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
